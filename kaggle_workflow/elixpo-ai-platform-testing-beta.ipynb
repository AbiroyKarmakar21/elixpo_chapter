{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.14","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":9419462,"sourceType":"datasetVersion","datasetId":5721048},{"sourceId":9421764,"sourceType":"datasetVersion","datasetId":5722748},{"sourceId":115445,"sourceType":"modelInstanceVersion","modelInstanceId":96973,"modelId":121158}],"dockerImageVersionId":30761,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Notebook Summary: Text Classification and Generation with Machine Learning\n\nThis notebook showcases various techniques for classifying and generating text using machine learning models. It includes implementations of text classification, data preprocessing, model training, saving and loading models, and generating descriptive text.\n\n## Table of Contents\n\n1. [Text Classification](#text-classification)\n    - [Logistic Regression Classifier](#logistic-regression-classifier)\n    - [Keras Model for Text Classification](#keras-model-for-text-classification)\n    - [Evaluation and Prediction](#evaluation-and-prediction)\n    - [Model Saving](#model-saving)\n2. [Text Generation](#text-generation)\n3. [Conclusion](#conclusion)\n\n## Text Classification\n\n### Logistic Regression Classifier\n\nThe first section implements a logistic regression classifier to differentiate between meaningful and gibberish text.\n\n- **Libraries Used**: `pandas`, `sklearn`, `pickle`\n- **Data Loading**: Combined dataset is loaded, consisting of two columns: `response` and `label`.\n- **Data Preprocessing**: The data is split into features (X) and labels (y), followed by a train-test split.\n- **Vectorization**: TF-IDF vectorization is applied to the text data.\n- **Model Training**: A logistic regression model is trained on the vectorized training data.\n- **Model Evaluation**: The model is evaluated on the test set, and the accuracy is printed.\n- **Model Saving**: The trained model and vectorizer are saved using pickle.\n\n### Keras Model for Text Classification\n\nThis section uses a Keras Sequential model for classifying text.\n\n- **Libraries Used**: `numpy`, `tensorflow.keras`, `sklearn`\n- **Data Preparation**: The dataset is prepared, and sequences are tokenized and padded.\n- **Model Definition**: A Keras model is defined with several dense layers and dropout for regularization.\n- **Model Compilation**: The model is compiled with Adam optimizer and binary cross-entropy loss.\n- **Training**: The model is trained for a specified number of epochs.\n- **Evaluation**: The model's predictions are evaluated using accuracy score and classification report.\n\n### Evaluation and Prediction\n\nIn this part, new texts are classified using the trained Keras model.\n\n- **Input Texts**: Example texts are defined for prediction.\n- **Tokenization and Padding**: The new texts are tokenized and padded to match the model's input shape.\n- **Predictions**: The model predicts whether the texts are meaningful or gibberish.\n\n### Model Saving\n\nThe Keras model and tokenizer are saved for future use.\n\n- **Keras Model**: Saved in HDF5 format.\n- **Tokenizer**: Saved using pickle for loading later.\n\n## Text Generation\n\nThis section utilizes the T5 model for generating text based on a descriptive prompt.\n\n- **Libraries Used**: `transformers`\n- **Model Loading**: The pre-trained T5 model and tokenizer are loaded.\n- **Input Prompt**: A specific input prompt is defined to guide the generation process.\n- **Text Generation**: The model generates text using sampling parameters like `top_k`, `top_p`, and `temperature`.\n- **Output**: The generated text is decoded and printed.\n\n## Conclusion\n\nThis notebook demonstrates a comprehensive workflow for text classification and generation using various models and techniques. It provides practical insights into loading data, preprocessing text, training machine learning models, and generating creative outputs. The techniques covered can be further extended or modified for more advanced applications in natural language processing.\n\n","metadata":{}},{"cell_type":"markdown","source":"# Gibberish vs Meaningful Text Classifier\n\nThis notebook trains a **Logistic Regression model** to classify text prompts as either gibberish or meaningful. The dataset includes two CSV files:\n\n1. **`non_sensical_combined.csv`** - Contains nonsensical or gibberish prompts.\n2. **`prompts_collection.csv`** - Contains meaningful prompts.\n\n### Steps:\n1. **Data Loading**: Load the datasets and combine them into one DataFrame.\n2. **Text Preprocessing**: Use **TF-IDF Vectorization** to transform the text into numerical features.\n3. **Model Training**: Train a **Logistic Regression** model on the processed text data.\n4. **Evaluation**: Evaluate the model using accuracy as a performance metric.\n5. **Model Saving**: Save the trained model and vectorizer as `.pkl` files for later use.\n\nThe goal is to classify text prompts accurately and save the trained model for future predictions.\n","metadata":{}},{"cell_type":"code","source":"import pandas as pd\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.metrics import accuracy_score\nimport pickle\n\n# Load datasets\nnonsensical_file = \"/kaggle/input/non-sensical-and-meaningful-prompts-ai-art/non_sensical_combined.csv\"\nprompt_collection_file = \"/kaggle/input/non-sensical-and-meaningful-prompts-ai-art/prompts_collection.csv\"\n\ndf_nonsensical = pd.read_csv(nonsensical_file)\ndf_prompt_collection = pd.read_csv(prompt_collection_file)\n\n# Combine the datasets\ndf_combined = pd.concat([df_nonsensical, df_prompt_collection])\n\n# Preprocessing: Vectorize the 'response' column\nvectorizer = TfidfVectorizer(max_features=5000)\nX = vectorizer.fit_transform(df_combined['response'].values.astype('U'))\n\n# Target (labels)\ny = df_combined['label']\n\n# Split the data\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n\n# Train a Logistic Regression model\nmodel = LogisticRegression()\nmodel.fit(X_train, y_train)\n\n# Evaluate the model\ny_pred = model.predict(X_test)\naccuracy = accuracy_score(y_test, y_pred)\nprint(f\"Model Accuracy: {accuracy * 100:.2f}%\")\n\n# Save the model as a pickle file\nwith open('gibberish_classifier.pkl', 'wb') as f:\n    pickle.dump(model, f)\n\n# Save the vectorizer for later use\nwith open('vectorizer.pkl', 'wb') as f:\n    pickle.dump(vectorizer, f)\n","metadata":{"execution":{"iopub.status.busy":"2024-09-17T10:14:31.269277Z","iopub.execute_input":"2024-09-17T10:14:31.269690Z","iopub.status.idle":"2024-09-17T10:19:06.321753Z","shell.execute_reply.started":"2024-09-17T10:14:31.269647Z","shell.execute_reply":"2024-09-17T10:19:06.320468Z"},"trusted":true},"execution_count":1,"outputs":[{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/sklearn/linear_model/_logistic.py:458: ConvergenceWarning: lbfgs failed to converge (status=1):\nSTOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n\nIncrease the number of iterations (max_iter) or scale the data as shown in:\n    https://scikit-learn.org/stable/modules/preprocessing.html\nPlease also refer to the documentation for alternative solver options:\n    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n  n_iter_i = _check_optimize_result(\n","output_type":"stream"},{"name":"stdout","text":"Model Accuracy: 99.51%\n","output_type":"stream"}]},{"cell_type":"markdown","source":"# Text Classification Using Pretrained Model\n\nThis section demonstrates how to load a pretrained **Logistic Regression model** and **TF-IDF vectorizer** to classify new text as either gibberish or meaningful.\n\n### Steps:\n1. **Load Pretrained Model**: Load the saved **Logistic Regression model** and **TF-IDF vectorizer** from `.pkl` files.\n2. **Text Classification Function**:\n   - The `classify_text()` function takes a new text input, vectorizes it using the loaded TF-IDF vectorizer, and predicts the label using the loaded model.\n   - The prediction is returned as either \"Meaningful\" or \"Gibberish\" based on the model's output.\n3. **Example**: An example input (`\"a car with its doors open in sunlight\"`) is classified using the `classify_text()` function.\n\nThis script enables the use of the pretrained model to classify any arbitrary text prompt.\n","metadata":{}},{"cell_type":"markdown","source":"","metadata":{}},{"cell_type":"code","source":"import pickle\nfrom sklearn.feature_extraction.text import TfidfVectorizer\n\n# Load the model and vectorizer\nwith open('/kaggle/working/gibberish_classifier.pkl', 'rb') as model_file:\n    model = pickle.load(model_file)\n\nwith open('/kaggle/working/vectorizer.pkl', 'rb') as vec_file:\n    vectorizer = pickle.load(vec_file)\n\n# Function to classify a new text\ndef classify_text(text):\n    # Vectorize the input text\n    text_vector = vectorizer.transform([text])\n    \n    # Predict using the loaded model\n    prediction = model.predict(text_vector)\n    print(prediction)\n    # Convert the prediction to a meaningful label\n    label = \"Meaningful\" if prediction[0] == 1 else \"Gibberish\"\n    \n    return label\n\n# Example usage\ninput_text = \"a car with it's doors open in sunlight\"\nresult = classify_text(input_text)\nprint(f\"The input text is classified as: {result}\")\n","metadata":{"execution":{"iopub.status.busy":"2024-09-17T10:23:39.923610Z","iopub.execute_input":"2024-09-17T10:23:39.924096Z","iopub.status.idle":"2024-09-17T10:23:40.071033Z","shell.execute_reply.started":"2024-09-17T10:23:39.924053Z","shell.execute_reply":"2024-09-17T10:23:40.069875Z"},"trusted":true},"execution_count":8,"outputs":[{"name":"stdout","text":"[1]\nThe input text is classified as: Meaningful\n","output_type":"stream"}]},{"cell_type":"markdown","source":"# Training a Logistic Regression Model for Gibberish vs Meaningful Text Classification\n\nThis notebook trains a **Logistic Regression** model to classify text as either gibberish or meaningful using the combined dataset.\n\n### Steps:\n1. **Data Loading**: \n   - Load the **gibberish** dataset (assumed `label = 0`).\n   - Load the **meaningful** dataset (assumed `label = 1`).\n   - Combine the datasets into a single DataFrame for processing.\n   \n2. **Data Shuffling**: \n   - Shuffle the combined dataset to ensure randomness and prevent any bias during training.\n\n3. **Text Preprocessing**: \n   - Split the combined dataset into input (`X`: the text) and target (`y`: the labels).\n   - Use **TF-IDF Vectorization** to convert the text data into numerical features.\n\n4. **Model Training**:\n   - Split the dataset into training and testing sets using an 80-20 ratio.\n   - Train a **Logistic Regression** model using the vectorized text data.\n\n5. **Evaluation**: \n   - Transform the test data using the trained vectorizer and evaluate the model's performance on unseen data.\n   - Display the accuracy of the model.\n\nThis workflow demonstrates how to classify text prompts as gibberish or meaningful using machine learning techniques.\n","metadata":{}},{"cell_type":"code","source":"import pandas as pd\n\n# Load the gibberish and meaningful datasets\ngibberish_df = pd.read_csv(r'/kaggle/input/non-sensical-and-meaningful-prompts-ai-art/non_sensical_combined.csv')  # Assumed label = 0\nmeaningful_df = pd.read_csv(r'/kaggle/input/non-sensical-and-meaningful-prompts-ai-art/prompts_collection.csv')  # Assumed label = 1\n\n# Combine the datasets\ncombined_df = pd.concat([gibberish_df, meaningful_df])\n\n# Shuffle the data\ncombined_df = combined_df.sample(frac=1).reset_index(drop=True)\n\n# Now use this combined_df for training\nX = combined_df['response']\ny = combined_df['label']\n\n# Split the data for training and testing\nfrom sklearn.model_selection import train_test_split\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n\n# Train your model\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nfrom sklearn.linear_model import LogisticRegression\n\n# Vectorize the text data\nvectorizer = TfidfVectorizer()\nX_train_vectorized = vectorizer.fit_transform(X_train)\n\n# Train the logistic regression model\nmodel = LogisticRegression(max_iter=500)\nmodel.fit(X_train_vectorized, y_train)\n\n# Evaluate on test data\nX_test_vectorized = vectorizer.transform(X_test)\naccuracy = model.score(X_test_vectorized, y_test)\nprint(f'Accuracy: {accuracy}')\n","metadata":{"execution":{"iopub.status.busy":"2024-09-17T10:25:36.918013Z","iopub.execute_input":"2024-09-17T10:25:36.918451Z","iopub.status.idle":"2024-09-17T10:32:45.485636Z","shell.execute_reply.started":"2024-09-17T10:25:36.918411Z","shell.execute_reply":"2024-09-17T10:32:45.484047Z"},"trusted":true},"execution_count":9,"outputs":[{"name":"stdout","text":"Accuracy: 0.9936159881825684\n","output_type":"stream"}]},{"cell_type":"markdown","source":"# Saving the Trained Model and Vectorizer\n\nThis section shows how to save the trained **Logistic Regression model** and the **TF-IDF vectorizer** for future use.\n\n### Steps:\n1. **Save the Model**: \n   - Use the `pickle` library to serialize and save the trained **Logistic Regression model** into a `.pkl` file (`gibberish_classifier2.pkl`).\n   \n2. **Save the Vectorizer**: \n   - Similarly, save the trained **TF-IDF vectorizer** into a separate `.pkl` file (`vectorizer2.pkl`).\n   \n3. **Confirmation**: \n   - A success message is printed to confirm that the model and vectorizer have been successfully saved.\n\nThese saved files can later be loaded to classify new text without retraining the model.\n","metadata":{}},{"cell_type":"code","source":"import pickle\n\n# Save the trained model\nwith open('gibberish_classifier2.pkl', 'wb') as model_file:\n    pickle.dump(model, model_file)\n\n# Save the vectorizer\nwith open('vectorizer2.pkl', 'wb') as vec_file:\n    pickle.dump(vectorizer, vec_file)\n\nprint(\"Model and vectorizer saved!\")\n","metadata":{"execution":{"iopub.status.busy":"2024-09-17T10:35:34.565172Z","iopub.execute_input":"2024-09-17T10:35:34.565699Z","iopub.status.idle":"2024-09-17T10:35:34.778065Z","shell.execute_reply.started":"2024-09-17T10:35:34.565649Z","shell.execute_reply":"2024-09-17T10:35:34.776766Z"},"trusted":true},"execution_count":10,"outputs":[{"name":"stdout","text":"Model and vectorizer saved!\n","output_type":"stream"}]},{"cell_type":"markdown","source":"# Loading and Using the Saved Model for Text Classification\n\nThis section demonstrates how to load a previously saved **Logistic Regression model** and **TF-IDF vectorizer** to classify new text inputs.\n\n### Steps:\n1. **Load the Model and Vectorizer**:\n   - Use `pickle` to load the trained **Logistic Regression model** (`gibberish_classifier2.pkl`) and the **TF-IDF vectorizer** (`vectorizer2.pkl`) from disk.\n\n2. **Text Classification Function**:\n   - The `classify_text()` function takes a text input, vectorizes it using the loaded vectorizer, and predicts the class using the loaded model.\n   - The prediction is converted to a meaningful label: \"Meaningful\" for class `1` and \"Gibberish\" for class `0`.\n\n3. **Example Usage**:\n   - An external text input (`\"a man with his arms spread\"`) is classified using the `classify_text()` function, and the result is printed.\n\nThis setup enables easy classification of new text prompts using the pretrained model and vectorizer.\n","metadata":{}},{"cell_type":"code","source":"import pickle\n\n# Load the model and vectorizer\nwith open('/kaggle/working/gibberish_classifier2.pkl', 'rb') as model_file:\n    model = pickle.load(model_file)\n\nwith open('/kaggle/working/vectorizer2.pkl', 'rb') as vec_file:\n    vectorizer = pickle.load(vec_file)\n\n# Function to classify a new text\ndef classify_text(text):\n    # Vectorize the input text\n    text_vector = vectorizer.transform([text])\n    \n    # Predict using the loaded model\n    prediction = model.predict(text_vector)\n    \n    # Convert the prediction to a meaningful label\n    label = \"Meaningful\" if prediction[0] == 1 else \"Gibberish\"\n    \n    return label\n\n# Example usage for external text\ninput_text = \"a man with his arms spread\"\nresult = classify_text(input_text)\nprint(f\"The input text is classified as: {result}\")\n","metadata":{"execution":{"iopub.status.busy":"2024-09-17T10:36:40.506752Z","iopub.execute_input":"2024-09-17T10:36:40.507190Z","iopub.status.idle":"2024-09-17T10:36:40.759784Z","shell.execute_reply.started":"2024-09-17T10:36:40.507150Z","shell.execute_reply":"2024-09-17T10:36:40.758572Z"},"trusted":true},"execution_count":11,"outputs":[{"name":"stdout","text":"The input text is classified as: Meaningful\n","output_type":"stream"}]},{"cell_type":"markdown","source":"# Combining and Saving the Gibberish and Meaningful Text Datasets\n\nThis section demonstrates how to combine two datasets (gibberish and meaningful prompts) and save the resulting dataset for further use.\n\n### Steps:\n1. **Load the Datasets**:\n   - Load the **gibberish** dataset (assumed to have `label = 0`).\n   - Load the **meaningful** dataset (assumed to have `label = 1`).\n   \n2. **Combine the Datasets**:\n   - Concatenate both datasets into a single DataFrame.\n\n3. **Save the Combined Dataset**:\n   - Save the combined dataset as `Combined.csv` for future use.\n\nThis ensures both gibberish and meaningful prompts are merged into one unified dataset, making it easier for training machine learning models.\n","metadata":{}},{"cell_type":"code","source":"import pandas as pd\n\n# Load the gibberish and meaningful datasets\ngibberish_df = pd.read_csv(r'/kaggle/input/non-sensical-and-meaningful-prompts-ai-art/non_sensical_combined.csv')  # Assuming label = 0\nmeaningful_df = pd.read_csv(r'/kaggle/input/non-sensical-and-meaningful-prompts-ai-art/prompts_collection.csv')  # Assuming label = 1\n\n# Combine both datasets\ncombined_df = pd.concat([gibberish_df, meaningful_df])\n\n# Save combined dataset as final.csv\ncombined_df.to_csv('Combined.csv', index=False)\n\nprint(\"Final dataset saved as final.csv\")\n","metadata":{"execution":{"iopub.status.busy":"2024-09-17T10:55:21.972743Z","iopub.execute_input":"2024-09-17T10:55:21.973668Z","iopub.status.idle":"2024-09-17T10:56:11.562332Z","shell.execute_reply.started":"2024-09-17T10:55:21.973623Z","shell.execute_reply":"2024-09-17T10:56:11.560857Z"},"trusted":true},"execution_count":12,"outputs":[{"name":"stdout","text":"Final dataset saved as final.csv\n","output_type":"stream"}]},{"cell_type":"markdown","source":"# Training and Saving a Logistic Regression Model for Text Classification\n\nThis section demonstrates the process of training a **Logistic Regression model** on a combined dataset of gibberish and meaningful text prompts, and saving the model along with the **TF-IDF vectorizer** for future use.\n\n### Steps:\n1. **Load the Combined Dataset**:\n   - Load the previously saved `Combined.csv` file, which contains both gibberish and meaningful text prompts with corresponding labels.\n\n2. **Data Splitting**:\n   - Split the dataset into features (`X`: the text) and labels (`y`: the classification labels).\n   - Further divide the data into training and testing sets (80% training, 20% testing).\n\n3. **Text Vectorization**:\n   - Use **TF-IDF Vectorization** to convert the text data into numerical features.\n\n4. **Model Training**:\n   - Train a **Logistic Regression** model using the vectorized training data.\n\n5. **Model Evaluation**:\n   - Transform the test data using the vectorizer and evaluate the model's performance by calculating its accuracy on the test set.\n\n6. **Saving the Model and Vectorizer**:\n   - Save the trained **Logistic Regression model** as `gibberish_classifier3.pkl`.\n   - Save the **TF-IDF vectorizer** as `vectorizer3.pkl`.\n\nThis process results in a trained model and vectorizer that can be used for classifying new text prompts as either gibberish or meaningful.\n","metadata":{}},{"cell_type":"code","source":"import pandas as pd\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.model_selection import train_test_split\nimport pickle\n\n# Load the combined dataset\ncombined_df = pd.read_csv('/kaggle/working/Combined.csv')\n\n# Split the data into features (X) and labels (y)\nX = combined_df['response']\ny = combined_df['label']\n\n# Split into training and testing sets\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n\n# Vectorize the text data\nvectorizer = TfidfVectorizer()\nX_train_vectorized = vectorizer.fit_transform(X_train)\n\n# Train the logistic regression model\nmodel = LogisticRegression(max_iter=500)\nmodel.fit(X_train_vectorized, y_train)\n\n# Evaluate the model on the test set\nX_test_vectorized = vectorizer.transform(X_test)\naccuracy = model.score(X_test_vectorized, y_test)\nprint(f'Accuracy: {accuracy}')\n\n# Save the model and vectorizer\nwith open('gibberish_classifier3.pkl', 'wb') as model_file:\n    pickle.dump(model, model_file)\n\nwith open('vectorizer3.pkl', 'wb') as vec_file:\n    pickle.dump(vectorizer, vec_file)\n\nprint(\"Model and vectorizer saved!\")\n","metadata":{"execution":{"iopub.status.busy":"2024-09-17T10:57:33.256101Z","iopub.execute_input":"2024-09-17T10:57:33.256575Z","iopub.status.idle":"2024-09-17T11:04:59.311190Z","shell.execute_reply.started":"2024-09-17T10:57:33.256515Z","shell.execute_reply":"2024-09-17T11:04:59.309925Z"},"trusted":true},"execution_count":14,"outputs":[{"name":"stdout","text":"Accuracy: 0.9935909126139228\nModel and vectorizer saved!\n","output_type":"stream"}]},{"cell_type":"markdown","source":"# Text Classification: Gibberish vs Meaningful Using a Pretrained Keras Model\n\nThis code showcases how to classify an input text as **gibberish** or **meaningful** using a **Keras deep learning model**. The model, trained on sequences of text, uses a tokenizer to process input text and classify it.\n\n### Key Components:\n1. **Loading the Pretrained Keras Model**:\n   - The Keras model is loaded from a saved `.h5` file, which was trained to distinguish between gibberish and meaningful text.\n\n2. **Loading the Tokenizer**:\n   - The tokenizer is loaded from a `tokenizer.pkl` file. This tokenizer is essential for converting the input text into sequences that the model can process.\n\n3. **Text Classification Function**:\n   - The `classify_text()` function accepts an input string, tokenizes and pads it to the required input length (`maxlen=100`), and then uses the model to predict whether the text is gibberish or meaningful.\n   - Predictions are made on the tokenized text, and the result is either `Meaningful` (for a class of 1) or `Gibberish` (for a class of 0).\n\n4. **User Interaction**:\n   - The script accepts a text input from the user, processes it through the `classify_text()` function, and then outputs whether the text is **Meaningful** or **Gibberish**.\n\n### Usage:\n- This code can be used to classify any given text in real-time, making it useful for detecting non-sensical text in natural language processing tasks.\n\n","metadata":{}},{"cell_type":"markdown","source":"","metadata":{}},{"cell_type":"code","source":"#1.0 classifier between normal text and gibberish text\nimport numpy as np\nimport pickle\nfrom tensorflow.keras.models import load_model\nfrom tensorflow.keras.preprocessing.sequence import pad_sequences\n\n# Load the trained Keras model\nmodel = load_model(r'/kaggle/input/test-classifier/keras/default/1/trained_model.h5')\n\n# Load the tokenizer\nwith open(r'/kaggle/input/test-classifier/keras/default/1/tokenizer.pkl', 'rb') as f:\n    tokenizer = pickle.load(f)\n\n# Function to classify new text\ndef classify_text(input_text):\n    # Tokenize and pad the input text\n    input_text_seq = tokenizer.texts_to_sequences([input_text])\n    input_text_padded = pad_sequences(input_text_seq, maxlen=100)  # Adjust maxlen as per your training setup\n\n    # Make prediction\n    prediction = model.predict(input_text_padded)\n\n    # Convert prediction to binary (0 or 1)\n    predicted_class = (prediction > 0.5).astype(\"int32\")\n\n    # Interpret the result\n    if predicted_class[0][0] == 1:\n        return \"Meaningful\"\n    else:\n        return \"Gibberish\"\n\n# Take user input\ninput_text = input(\"Enter a text: \")\n\n# Get prediction\nresult = classify_text(input_text)\n\n# Display the result\nprint(f\"Text: {input_text}\\nPredicted Class: {result}\")\n","metadata":{"execution":{"iopub.status.busy":"2024-09-17T11:35:56.095727Z","iopub.execute_input":"2024-09-17T11:35:56.096868Z","iopub.status.idle":"2024-09-17T11:35:59.634376Z","shell.execute_reply.started":"2024-09-17T11:35:56.096817Z","shell.execute_reply":"2024-09-17T11:35:59.633239Z"},"trusted":true},"execution_count":20,"outputs":[{"output_type":"stream","name":"stdin","text":"Enter a text:  dwdwhdhgadgdw\n"},{"name":"stdout","text":"\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 63ms/step\nText: dwdwhdhgadgdw\nPredicted Class: Gibberish\n","output_type":"stream"}]},{"cell_type":"markdown","source":"# Text Classification Using a Neural Network: Gibberish vs Meaningful Text\n\nThis Python script classifies text into **gibberish** or **meaningful** using a **neural network** built with TensorFlow and Keras. It processes the text data, trains a model, and evaluates its performance on a test dataset.\n\n### Key Components:\n1. **Loading the Dataset**:\n   - The dataset is loaded from a CSV file, and it is assumed that the file contains two columns: `response` (the text data) and `label` (the target labels, where 0 indicates gibberish and 1 indicates meaningful text).\n\n2. **Splitting the Data**:\n   - The dataset is split into training and test sets using an 80/20 split.\n\n3. **Text Preprocessing**:\n   - The text data is tokenized using Keras's `Tokenizer`, which converts the text into sequences of integers, where each integer represents a word. \n   - These sequences are then padded to ensure uniform length (100 in this case).\n\n4. **Building the Neural Network Model**:\n   - A **Sequential model** is defined, which includes:\n     - A dense layer with 64 units and ReLU activation.\n     - A dropout layer to prevent overfitting.\n     - Another dense layer with 32 units and ReLU activation.\n     - A final output layer with a sigmoid activation for binary classification.\n   \n5. **Compiling the Model**:\n   - The model uses the **Adam optimizer** and **binary crossentropy loss function** for training, which is appropriate for binary classification problems.\n   - The `accuracy` metric is used to evaluate the model’s performance.\n\n6. **Training the Model**:\n   - The model is trained for 5 epochs with a batch size of 32, and 10% of the training data is used for validation during training.\n\n7. **Evaluating the Model**:\n   - The model's predictions on the test set are compared to the true labels, and the classification accuracy and a detailed classification report (precision, recall, F1-score) are printed.\n\n### Usage:\n- This code provides an efficient way to train a binary classifier that distinguishes between gibberish and meaningful text. It is suitable for various natural language processing tasks where text quality needs to be evaluated.\n\n### Dependencies:\n- pandas\n- scikit-learn\n- tensorflow/keras\n\n","metadata":{}},{"cell_type":"code","source":"import pandas as pd\nfrom sklearn.model_selection import train_test_split\nimport tensorflow as tf\nfrom tensorflow.keras.models import Sequential\nfrom tensorflow.keras.layers import Dense, Dropout\nfrom tensorflow.keras.preprocessing.text import Tokenizer\nfrom tensorflow.keras.preprocessing.sequence import pad_sequences\nfrom sklearn.metrics import accuracy_score, classification_report\n\n# Load dataset\ndf = pd.read_csv('/kaggle/input/prompt-and-gibberish-for-ai-art-gen/Combined-non_sensical-meaningful.csv')\n\n# Assuming the dataset has columns 'response' and 'label'\nX = df['response']\ny = df['label']\n\n# Split the dataset\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n\n# Convert all entries in X_train and X_test to strings\nX_train = X_train.astype(str)\nX_test = X_test.astype(str)\n\n# Tokenize and pad sequences\ntokenizer = Tokenizer(num_words=5000)\ntokenizer.fit_on_texts(X_train)\nX_train_seq = tokenizer.texts_to_sequences(X_train)\nX_test_seq = tokenizer.texts_to_sequences(X_test)\nX_train_padded = pad_sequences(X_train_seq, maxlen=100)\nX_test_padded = pad_sequences(X_test_seq, maxlen=100)\n\n# Define the model\nmodel = Sequential()\nmodel.add(Dense(64, activation='relu', input_shape=(100,)))\nmodel.add(Dropout(0.5))\nmodel.add(Dense(32, activation='relu'))\nmodel.add(Dense(1, activation='sigmoid'))\n\n# Compile the model\nmodel.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n\n# Train the model\nmodel.fit(X_train_padded, y_train, epochs=5, batch_size=32, validation_split=0.1)\n\n# Evaluate the model\ny_pred = (model.predict(X_test_padded) > 0.5).astype(\"int32\")\nprint(\"Accuracy:\", accuracy_score(y_test, y_pred))\nprint(\"Classification Report:\\n\", classification_report(y_test, y_pred))\n","metadata":{"execution":{"iopub.status.busy":"2024-09-17T12:09:53.037279Z","iopub.execute_input":"2024-09-17T12:09:53.037801Z","iopub.status.idle":"2024-09-17T12:31:52.969444Z","shell.execute_reply.started":"2024-09-17T12:09:53.037758Z","shell.execute_reply":"2024-09-17T12:31:52.968144Z"},"trusted":true},"execution_count":24,"outputs":[{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/keras/src/layers/core/dense.py:87: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n","output_type":"stream"},{"name":"stdout","text":"Epoch 1/5\n\u001b[1m98702/98702\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m188s\u001b[0m 2ms/step - accuracy: 0.9606 - loss: 1.0841 - val_accuracy: 0.9756 - val_loss: 0.0717\nEpoch 2/5\n\u001b[1m98702/98702\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m186s\u001b[0m 2ms/step - accuracy: 0.9756 - loss: 0.0769 - val_accuracy: 0.9786 - val_loss: 0.0664\nEpoch 3/5\n\u001b[1m98702/98702\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m189s\u001b[0m 2ms/step - accuracy: 0.9767 - loss: 0.0744 - val_accuracy: 0.9775 - val_loss: 0.0783\nEpoch 4/5\n\u001b[1m98702/98702\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m195s\u001b[0m 2ms/step - accuracy: 0.9769 - loss: 0.0745 - val_accuracy: 0.9801 - val_loss: 0.0692\nEpoch 5/5\n\u001b[1m98702/98702\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m185s\u001b[0m 2ms/step - accuracy: 0.9768 - loss: 0.0715 - val_accuracy: 0.9793 - val_loss: 0.0675\n\u001b[1m27418/27418\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m33s\u001b[0m 1ms/step\nAccuracy: 0.9794175173363363\nClassification Report:\n               precision    recall  f1-score   support\n\n           0       0.95      0.97      0.96    210075\n           1       0.99      0.98      0.99    667273\n\n    accuracy                           0.98    877348\n   macro avg       0.97      0.98      0.97    877348\nweighted avg       0.98      0.98      0.98    877348\n\n","output_type":"stream"}]},{"cell_type":"markdown","source":"# Classifying New Text Data: Gibberish vs Meaningful Text\n\nThis code snippet demonstrates how to classify new text inputs as either **gibberish** or **meaningful** using a pre-trained neural network model.\n\n### Key Components:\n\n1. **New Text Data**:\n   - A list of new text samples is defined for classification. For example:\n     ```python\n     new_texts = [\n         \"sunflower fields in a valley in sunlight\",\n         \"fefefe grgsgr\"\n     ]\n     ```\n\n2. **Text Preprocessing**:\n   - The new texts are tokenized using the previously defined `tokenizer`, converting each text sample into a sequence of integers that represent words.\n   - The sequences are then padded to ensure they all have the same length (100 in this case) using the `pad_sequences` function from Keras.\n\n3. **Making Predictions**:\n   - The model, which has been trained on the dataset, is used to predict the classes of the new padded sequences.\n   - Predictions are made using the `model.predict()` method, which outputs a probability score for each class.\n\n4. **Converting Predictions to Binary**:\n   - The output predictions are converted to binary values (0 or 1) based on a threshold of 0.5. \n   - If the predicted score is greater than 0.5, it is classified as `1` (meaningful); otherwise, it is classified as `0` (gibberish).\n\n5. **Displaying Predictions**:\n   - The results are printed, showing each text alongside its predicted class. For example:\n     ```python\n     for text, pred in zip(new_texts, predicted_classes):\n         print(f\"Text: {text}\\nPredicted Class: {'Meaningful' if pred[0] == 1 else 'Gibberish'}\\n\")\n     ```\n\n### Usage:\n- This code can be used to evaluate any new text inputs against a previously trained model, allowing users to quickly identify whether the text is meaningful or gibberish.\n\n### Dependencies:\n- This snippet assumes that the necessary libraries (e.g., TensorFlow, Keras) and the trained model and tokenizer are already imported and set up in the environment.\n","metadata":{}},{"cell_type":"code","source":"# Example new text data\nnew_texts = [\n    \"sunflower fields in a valley in sunlight\",\n    \"fefefe grgsgr\"\n]\n\n# Tokenize and pad the new texts\nnew_texts_seq = tokenizer.texts_to_sequences(new_texts)\nnew_texts_padded = pad_sequences(new_texts_seq, maxlen=100)\n\n# Make predictions\npredictions = model.predict(new_texts_padded)\n\n# Convert predictions to binary (0 or 1)\npredicted_classes = (predictions > 0.5).astype(\"int32\")\n\n# Display predictions\nfor text, pred in zip(new_texts, predicted_classes):\n    print(f\"Text: {text}\\nPredicted Class: {'Meaningful' if pred[0] == 1 else 'Gibberish'}\\n\")\n","metadata":{"execution":{"iopub.status.busy":"2024-09-17T12:43:13.187467Z","iopub.execute_input":"2024-09-17T12:43:13.188299Z","iopub.status.idle":"2024-09-17T12:43:13.289029Z","shell.execute_reply.started":"2024-09-17T12:43:13.188251Z","shell.execute_reply":"2024-09-17T12:43:13.287905Z"},"trusted":true},"execution_count":40,"outputs":[{"name":"stdout","text":"\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 29ms/step\nText: sunflower fields in a valley in sunlight\nPredicted Class: Gibberish\n\nText: fefefe grgsgr\nPredicted Class: Gibberish\n\n","output_type":"stream"}]},{"cell_type":"markdown","source":"# Saving the Keras Model and Tokenizer\n\nThis code snippet demonstrates how to save a trained Keras model and its associated tokenizer for later use.\n\n### Key Components:\n\n1. **Saving the Keras Model**:\n   - The trained model is saved in HDF5 format using the `save` method provided by Keras. This allows for easy loading and inference later.\n   ```python\n   model.save('prompt_classifier_model.h5')  # Save Keras model in HDF5 format\n","metadata":{}},{"cell_type":"code","source":"import pickle\n\n# Save the Keras model\nmodel.save('prompt_classifier_model.h5')  # Save Keras model in HDF5 format\n\n# Optionally, you can also save the tokenizer\nwith open('prompt_classifier_tokenizer.pkl', 'wb') as f:\n    pickle.dump(tokenizer, f)\n","metadata":{"execution":{"iopub.status.busy":"2024-09-17T12:37:13.311795Z","iopub.execute_input":"2024-09-17T12:37:13.312273Z","iopub.status.idle":"2024-09-17T12:37:14.199516Z","shell.execute_reply.started":"2024-09-17T12:37:13.312227Z","shell.execute_reply":"2024-09-17T12:37:14.197386Z"},"trusted":true},"execution_count":26,"outputs":[]},{"cell_type":"markdown","source":"# Text Classifier for Meaningful vs. Gibberish Text\n\nThis Python script implements a classifier that distinguishes between meaningful text and gibberish using a pre-trained Keras model.\n\n## Overview\n\nThe script loads a trained Keras model and a tokenizer, processes input text, and outputs whether the text is meaningful or gibberish. This can be useful for applications like chatbots, content moderation, or text validation.\n\n## Code Breakdown\n\n### 1. Importing Libraries\n\nThe script begins by importing necessary libraries:\n\n```python\nimport numpy as np\nimport pickle\nfrom tensorflow.keras.models import load_model\nfrom tensorflow.keras.preprocessing.sequence import pad_sequences\n##Loading the model\nmodel = load_model(r'/kaggle/input/test-classifier/keras/default/1/trained_model.h5')\n## Loading the tokenizer \nwith open(r'/kaggle/input/test-classifier/keras/default/1/tokenizer.pkl', 'rb') as f:\n    tokenizer = pickle.load(f)\n","metadata":{}},{"cell_type":"code","source":"#1.0 classifier between normal text and gibberish text\nimport numpy as np\nimport pickle\nfrom tensorflow.keras.models import load_model\nfrom tensorflow.keras.preprocessing.sequence import pad_sequences\n\n# Load the trained Keras model\nmodel = load_model(r'/kaggle/input/test-classifier/keras/default/1/trained_model.h5')\n\n# Load the tokenizer\nwith open(r'/kaggle/input/test-classifier/keras/default/1/tokenizer.pkl', 'rb') as f:\n    tokenizer = pickle.load(f)\n\n# Function to classify new text\ndef classify_text(input_text):\n    # Tokenize and pad the input text\n    input_text_seq = tokenizer.texts_to_sequences([input_text])\n    input_text_padded = pad_sequences(input_text_seq, maxlen=100)  # Adjust maxlen as per your training setup\n\n    # Make prediction\n    prediction = model.predict(input_text_padded)\n\n    # Convert prediction to binary (0 or 1)\n    predicted_class = (prediction > 0.5).astype(\"int32\")\n\n    # Interpret the result\n    if predicted_class[0][0] == 1:\n        return \"Meaningful\"\n    else:\n        return \"Gibberish\"\n\n# Take user input\ninput_text = input(\"Enter a text: \")\n\n# Get prediction\nresult = classify_text(input_text)\n\n# Display the result\nprint(f\"Text: {input_text}\\nPredicted Class: {result}\")\n","metadata":{"execution":{"iopub.status.busy":"2024-09-17T12:43:19.399565Z","iopub.execute_input":"2024-09-17T12:43:19.400639Z","iopub.status.idle":"2024-09-17T12:43:31.327840Z","shell.execute_reply.started":"2024-09-17T12:43:19.400577Z","shell.execute_reply":"2024-09-17T12:43:31.326762Z"},"trusted":true},"execution_count":41,"outputs":[{"output_type":"stream","name":"stdin","text":"Enter a text:  sunflower field in the valley in sunlight\n"},{"name":"stdout","text":"\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 74ms/step\nText: sunflower field in the valley in sunlight\nPredicted Class: Meaningful\n","output_type":"stream"}]}]}